# -*- coding: utf-8 -*-
"""불황과 호황 예측하기.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v8DcIxgAPP6OcVBZ7vSKxQ5z7PnqsW4R
"""

import datetime
import requests
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup

key = 'key'
url = 'https://ecos.bok.or.kr/api/StatisticTableList/'+key+'/xml/kr/1/10000'
raw = requests.get(url)
xml = BeautifulSoup(raw.text, 'xml')
raw_data = xml.find_all('row')

data = []
for i in range(len(raw_data)):
    p_stat_code = raw_data[i].P_STAT_CODE.string.strip()
    stat_code = raw_data[i].STAT_CODE.string.strip()
    stat_name = raw_data[i].STAT_NAME.string.strip()
    cycle = raw_data[i].find('CYCLE').text
    used = raw_data[i].SRCH_YN.string.strip()
    org_name = raw_data[i].ORG_NAME.string

    total=[p_stat_code, stat_code, stat_name, cycle, used, org_name]

    data.append(total)

df = pd.DataFrame(data, columns=['p_stat_code','stat_code','stat_name','cycle','used','org_name'])
df.to_csv("bok_total_list.csv", encoding='CP949')

df1 = df[df['used'].isin(['Y'])]
stat_code = df1['stat_code'].tolist()
len(stat_code)

# 세부 통계목록
data=[]

for i in range(len(stat_code)):
    code = stat_code[i]
    url = 'https://ecos.bok.or.kr/api/StatisticItemList/'+key+'/xml/kr/1/100/'+str(code)+'/'
    raw = requests.get(url)
    xml = BeautifulSoup(raw.text, 'xml')
    raw_data = xml.find_all('row')

    for j in range(len(raw_data)):
        stat_code1 = raw_data[j].STAT_CODE.string.strip()
        stat_name = raw_data[j].STAT_NAME.string.strip()
        grp_code = raw_data[j].GRP_CODE.string.strip()
        grp_name = raw_data[j].GRP_NAME.string.strip()
        item_code = raw_data[j].ITEM_CODE.string.strip()
        item_name = raw_data[j].ITEM_NAME.string.strip()
        cycle = raw_data[j].find("CYCLE").text
        start_time = raw_data[j].START_TIME.string.strip()
        end_time = raw_data[j].END_TIME.string.strip()
        data_cnt = raw_data[j].DATA_CNT.string.strip()

        total = [stat_code1,stat_name,grp_code,grp_name,item_code,item_name,cycle,start_time,end_time,data_cnt]
        data.append(total)

temp = pd.DataFrame(data, columns=['stat_code','stat_name','grp_code','grp_name','item_code','item_name','cycle','start_time','end_time','data_cnt'])
temp.to_csv('kob.detailTotal.csv', encoding='CP949')

df = temp.copy()
df1 = df[df['stat_code'].isin(['101Y001'])] # M2상품변 구성내역 말잔(계정조정)
df1

main_df = pd.read_csv('bok_total_list.csv', encoding='CP949')
detail_df = pd.read_csv('kob.detailTotal.csv', encoding='CP949')

def EcosDownload(statname, statcode, freq, begdate, enddate, item_code, subcode1, subcode2, subcode3, col_name):
    url = "https://ecos.bok.or.kr/api/StatisticSearch/"+key+"/xml/kr/1/1000/%s/%s/%s/%s/%s/%s/%s/%s"%( statcode, freq, begdate, enddate, item_code, subcode1, subcode2, subcode3)

    print(url)

    raw=requests.get(url)
    xml = BeautifulSoup(raw.text, 'xml')
    raw_data = xml.find_all('row')
    data_list =[]
    value_list = []

    for item in raw_data:
        value = item.find('DATA_VALUE').text.encode('utf-8')
        data_str = item.find('TIME').text
        
        if 'Q1' in data_str:
            data_str = data_str.replace('Q1', '03')
        if 'Q2' in data_str:
            data_str = data_str.replace('Q2', '06')
        if 'Q3' in data_str:
            data_str = data_str.replace('Q3', '09')
        if 'Q4' in data_str:
            data_str = data_str.replace('Q4', '12')

        value = float(value)
        data_list.append(data_str)    
        value_list.append(value)

    df = pd.DataFrame(index=data_list)
    df['%s'%(col_name)] = value_list
    
    return df

"""#불황과 호황 예측
- 로짓분석 머신러닝 사용
- 거지경제 데이터 이용
- 데이터 = 한국은행경졔통계 시스템 사용

## 불황과 호황을 예측하는데 이용되는 데이터 
 - 호황과 불황을 나타내는 이진목표변수 <- realGDP, 12분기 이동평균을 초과하면 호황(1)을 할당. 그렇지 않다면 불황(0) 할당
 - 변수 
    - realGDP : 실질 국내 총생산(단위 : 전분기 대비 증가율)
    - RealCons : 실질 민간소비(단위 : 전분기 대비 증가율)
    - INV : 총투자(단위 : 전분기 대비 증가율)
    - M2 : M2 통화량(단위 : 전분기 대비 증가율)
    - UNEMP : 실업률(단위 : 현분기 실업율)
    - EMPLOY : 취업자수(단위 : 전분기 대비 증가율)
    - CD_3M : CD 3개월 유통 수익률(단위 : 현분기 수준)
    - INFL : 소비자 물가(단위 : 전분기 대비 증가율)
"""

# 데이터 찾기...
detail_df[(detail_df['stat_code'].str.contains('200Y056') 
& detail_df['cycle'].str.contains('Q') )]

# begdate, enddate 고정 :2015Q1, 2022Q4
# 총투자 begdate가 2015Q1에서 시작
d_tmp = [['realGDP','2.1.1.2. 주요지표(분기지표)', '200Y002', '10111'], 
['realCons','2.1.1.2. 주요지표(분기지표)', '200Y002', '10122'], 
['inv','2.1.9.2. 총저축과 총투자(원계열, 명목, 분기 및 연간)', '200Y056', '13201'], 
['M2','1.1.3.1.2. M2 상품별 구성내역(평잔, 원계열)', '101Y004', 'BBHA01'], 
['unemp','9.1.5.2. 국제 주요국 실업률(계절변동조정)', '902Y021', 'KOR'], 
['employ','9.1.5.3. 국제 주요국 취업자수(계절변동조정)', '902Y022', 'KOR'], 
['CD_3M','1.3.2.2. 시장금리(월,분기,년)', '721Y001', '2010000'], 
['infl','9.1.2.2. 국제 주요국 소비자물가지수', '902Y008', 'KR']]

tmp_data = pd.DataFrame([])

for i in range(len(d_tmp)):
    tmp = (EcosDownload(str(d_tmp[i][1]),  str(d_tmp[i][2]), 'Q', '2015Q1', '2022Q4',  str(d_tmp[i][3]), '', '', '', d_tmp[i][0]))
    tmp_data = pd.concat([tmp_data,tmp],axis=1)

tmp_data

data = tmp_data.copy()
data['index'] = list(map(int, data.index))
data

data['QUARTER'] = ((data['index'] % 100)/3).astype(int) # % 나머지 
data['RollingMean']= data.realGDP.rolling(12).mean()
data['TARGET1'] = (data.realGDP > data.RollingMean).astype(int).shift(-1)
pct_cols = ['M2', 'infl']
data.loc[:, pct_cols] = data.loc[:, pct_cols].pct_change(1)
df = pd.get_dummies(data, columns=['QUARTER'], drop_first=True).dropna()

df.TARGET1.value_counts()

df1 = df.copy()

x_data =df1[['realGDP', 'realCons', 'inv', 'M2', 'infl', 'unemp', 'employ', 'CD_3M']].to_numpy()

y_data = df1.TARGET1

def normalization(data):
    numerator = data - np.min(data, 0)
    denominator = np.max(data, 0) - np.min(data, 0)
    return numerator / denominator

x_data = normalization(x_data)

#convert into numpy and float format
X = np.asarray(x_data, dtype=np.float32)
y = np.asarray(y_data, dtype=np.float32)

k = x_data.shape[1]

import tensorflow as tf

learning_rate = tf.Variable(0.003)

W = tf.Variable(tf.random.normal(([k, 1])), name='weight')
b = tf.Variable(tf.random.normal(([1])), name='bias')

for i in range(10000+1):
    with tf.GradientTape() as tape:
        
        hypothesis  = tf.sigmoid(tf.matmul(X, W) + b)
        
        cost = -tf.reduce_mean(y * tf.math.log(hypothesis) + (1 - y) * tf.math.log(1 - hypothesis))

        W_grad, b_grad = tape.gradient(cost, [W, b])
        
        W.assign_sub(learning_rate * W_grad)
        b.assign_sub(learning_rate * b_grad)
        predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)    
        
    if i % 2000 == 0:
        print("{:5} | {:10.6f}".format(i, cost.numpy()))

y_Predicted = predicted.numpy().flatten()

y_Actual = y.flatten()

data = {'y_Actual': y_Actual,
        'y_Predicted': y_Predicted}

df = pd.DataFrame(data, columns = ['y_Actual', 'y_Predicted'])

cross = pd.crosstab(df['y_Actual'], df['y_Predicted'], rownames = ['Actual'], colnames=['Predicted'])
cross

confusion_matrix = np.zeros([2,2])

try : 
    confusion_matrix[1,1] = cross.loc[1,1]
    confusion_matrix[0,1] = cross.loc[0,1]
    confusion_matrix[1,0] = cross.loc[1,0]
    confusion_matrix[0,0] = cross.loc[0,0]

except Exception as e:
    print(e)

TP  = confusion_matrix[1,1]
FP  = confusion_matrix[0,1]
FN  = confusion_matrix[1,0]
TN  = confusion_matrix[0,0]

confusion_matrix

TOT  = TP + FP + TN + FN

accuracy = (TP + TN)/TOT
accuracy

